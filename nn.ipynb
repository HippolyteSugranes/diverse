{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rdm\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(X):\n",
    "    return np.maximum(0,X)\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def relu_prime(X):\n",
    "    return np.array([1 if x >= 0 else 0 for x in X])\n",
    "\n",
    "def activation(name):\n",
    "    if name =='sigmoid':\n",
    "        return sigmoid,sigmoid_prime\n",
    "    elif name=='relu':\n",
    "        return relu,relu_prime\n",
    "    else:\n",
    "        raise Exception('activation function not in [sigmoid,relu]')\n",
    "        \n",
    "def mean_square_error(y_hat,y):\n",
    "    return np.square(y_hat - y).mean(axis = 1)\n",
    "\n",
    "def mean_square_error_der(y_hat,y):\n",
    "    if len(y) > 1:\n",
    "        return 2*(y_hat - y).mean(axis = 1)\n",
    "    else:\n",
    "        return 2*(y_hat - y)\n",
    "\n",
    "def square_sum(X):\n",
    "    return np.square(X).sum()\n",
    "\n",
    "def loss_function(name):\n",
    "    if name=='mean_square_error':\n",
    "        return mean_square_error , mean_square_error_der\n",
    "    else:\n",
    "        raise Exception('loss function not in [mean_square_error]')\n",
    "\n",
    "def penalization(name):\n",
    "    if name == 'square_sum':\n",
    "        return square_sum\n",
    "    else:\n",
    "        raise Exception('penalization function not in [square_sum]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,layers_parameters , global_parameters):\n",
    "        self.layers = {'layer_' + str(c):\n",
    "                         {'W' : 0.1*np.random.random(size = (layers[l]['output_dim'],layers[l]['input_dim'])),\n",
    "                          'B' : 0.1*np.random.random(size = (layers[l]['output_dim'],1)),\n",
    "                          'g' : activation(layers[l]['activation'])[0],\n",
    "                          'g_prime': activation(layers[l]['activation'])[1],\n",
    "                          'shape': {'W':(layers[l]['output_dim'],layers[l]['input_dim']),\n",
    "                                    'B':(layers[l]['output_dim'],1),\n",
    "                                    'g_prime':(layers[l]['output_dim'],1),\n",
    "                                    'input':(layers[l]['input_dim'],1),\n",
    "                                    'output':(layers[l]['output_dim'],1)}\n",
    "                         } for c,l in enumerate(layers)}\n",
    "        \n",
    "        self.input_dim = global_parameters['input']\n",
    "        self.output_dim = global_parameters['output']\n",
    "        self.layers_dim = global_parameters['layers']\n",
    "        self.batch_size = global_parameters['batch']\n",
    "        self.learning_rate = global_parameters['learning_rate']\n",
    "        self.train_test_split = parameters['train_test_split']\n",
    "        \n",
    "        self.forward_memory = {}\n",
    "        self.backward_memory= {}\n",
    "        \n",
    "        self.loss = loss_function(global_parameters['loss'])[0]\n",
    "        self.loss_prime = loss_function(global_parameters['loss'])[1]\n",
    "        \n",
    "    def predict(self,X):\n",
    "        #doit gérer les entrées en tableau\n",
    "        return np.array([self.forward(x) for x in X]).reshape((len(X),self.output_dim))\n",
    "    \n",
    "    def loss_calculation(self,x,y):\n",
    "        y_hat = self.predict(X)\n",
    "        return self.loss(y_hat,h)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.foward_memory = {}\n",
    "        \n",
    "        if len(x) != self.input_dim:\n",
    "            raise Exception('input of size {} instead of {}'.format(len(x),self.input_dim))\n",
    "            \n",
    "        x = x.reshape((self.input_dim,1))\n",
    "        \n",
    "        for n in range(self.layers_dim):\n",
    "            name = 'layer_' + str(n)\n",
    "            z = np.dot(self.layers[name]['W'] , x) + self.layers[name]['B'].reshape(self.layers[name]['shape']['output'])\n",
    "            h = self.layers[name]['g'](z).reshape(self.layers[name]['shape']['output'])\n",
    "            x = x.reshape(self.layers[name]['shape']['input'])\n",
    "            \n",
    "            self.forward_memory[name] = [x , z , h]\n",
    "            \n",
    "            x = np.array(h)\n",
    "            \n",
    "        return h\n",
    "    \n",
    "    def backward(self , y):\n",
    "        \n",
    "        if len(self.forward_memory) != self.layers_dim:\n",
    "            raise Exception('dimensions do not match')\n",
    "        \n",
    "        last_layer = 'layer_' + str(self.layers_dim - 1)\n",
    "        \n",
    "        dJ_dh = self.loss_prime(self.forward_memory[last_layer][2] , y.reshape((2,1)))\n",
    "        dJ_dh.reshape((self.output_dim , 1))\n",
    "        \n",
    "        for i in range(self.layers_dim):\n",
    "            current_layer_name = 'layer_' + str(self.layers_dim - i - 1)\n",
    "            x , z , h = self.forward_memory[current_layer_name]\n",
    "            n , m = self.layers[current_layer_name]['shape']['output'] , self.layers[current_layer_name]['shape']['input']\n",
    "            \n",
    "            if x.shape != m or z.shape != n or h.shape != n:\n",
    "                x.reshape(m)\n",
    "                z.reshape(n)\n",
    "                h.reshape(n)\n",
    "            \n",
    "            dJ_dz = dJ_dh * self.layers[current_layer_name]['g_prime'](z)\n",
    "            \n",
    "            if dJ_dz.shape != n:\n",
    "                dJ_dz = dJ_dz.reshape(n)\n",
    "                \n",
    "            dJ_dw = np.dot(dJ_dz,x.T)\n",
    "            dJ_db = dJ_dz\n",
    "            \n",
    "            self.backward_memory[current_layer_name] = {'dJ_dw':dJ_dw,'dJ_db':dJ_db}\n",
    "            \n",
    "            dJ_dh = np.dot(self.layers[current_layer_name]['W'].T,dJ_dz)\n",
    "                        \n",
    "        return True\n",
    "    \n",
    "    def train(self , X , Y , epoch):\n",
    "        if len(X) != len(Y):\n",
    "            raise Exception('X and Y dimension do not match')\n",
    "        \n",
    "        sc = self.score(X,Y)\n",
    "        print('initial score: ',sc,end = '\\n')\n",
    "        for l in range(epoch):\n",
    "            t0 = time.time()\n",
    "            data = list(zip(X, Y))\n",
    "            rdm.shuffle(data)\n",
    "            X, Y = zip(*data)\n",
    "            X_test = X[:int(self.train_test_split*len(X))]\n",
    "            Y_test = Y[:int(self.train_test_split*len(X))]\n",
    "            X_train = X[int(self.train_test_split*len(X)):]\n",
    "            Y_train = Y[int(self.train_test_split*len(X)):]\n",
    "            \n",
    "            n = int(len(X_train)/self.batch_size)\n",
    "            for i in range(n):\n",
    "                gradient_update = {}\n",
    "                for k in range(self.batch_size):\n",
    "                    current_element = i*self.batch_size + k\n",
    "                    x , y = X_train[current_element] , Y_train[current_element]\n",
    "                    temp = self.forward(x)\n",
    "                    self.backward(y)\n",
    "                    if k==0:\n",
    "                        gradient_update = self.backward_memory\n",
    "                    else:\n",
    "                        for layer in self.backward_memory:\n",
    "                            gradient_update[layer]['dJ_dw'] += self.backward_memory[layer]['dJ_dw']\n",
    "                            gradient_update[layer]['dJ_db'] += self.backward_memory[layer]['dJ_db']\n",
    "                \n",
    "                for layer in self.layers:\n",
    "                    self.layers[layer]['W'] -= self.learning_rate/self.batch_size * gradient_update[layer]['dJ_dw']\n",
    "                    self.layers[layer]['B'] -= self.learning_rate/self.batch_size * gradient_update[layer]['dJ_db']\n",
    "            \n",
    "            \n",
    "            train_score = self.score(X_train,Y_train)\n",
    "            test_score = self.score(X_test,Y_test)\n",
    "            t = time.time() - t0\n",
    "            results = 'epoch n°{} realized, training score: {}, test score: {}, time: {}'.format(l,train_score,test_score,t)\n",
    "            print(results, end = '\\n')\n",
    "            \n",
    "    def score(self,X,Y):\n",
    "        return np.sqrt(np.square(self.predict(X) - Y).mean(axis = 1).mean(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = {\n",
    "          0:{'input_dim' : 2,\n",
    "             'output_dim': 3,\n",
    "             'activation': 'sigmoid'},\n",
    "          1:{'input_dim' : 3,\n",
    "             'output_dim': 2,\n",
    "             'activation': 'relu'}\n",
    "         }\n",
    "\n",
    "parameters = {'input': 2,\n",
    "              'output' : 2,\n",
    "              'layers': 2,\n",
    "              'loss': 'mean_square_error',\n",
    "              'penalization':'square_sum',\n",
    "              'batch':1,\n",
    "              'learning_rate':0.1,\n",
    "              'train_test_split':0.25}\n",
    "\n",
    "nn = NeuralNetwork(layers , parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# for i in range(10000):\n",
    "#     y = i%2\n",
    "#     dist = np.random.normal(loc = 1 + i%2,scale = 0.2)\n",
    "#     x0 = np.random.uniform(low = -np.sqrt(dist/2),high = np.sqrt(dist/2))\n",
    "#     sign = -1 if np.random.random() > 0.5 else 1\n",
    "#     x1 = sign*np.sqrt(dist - x0**2)\n",
    "#     data.append([x0,x1,y])\n",
    "# X = np.array(data)[:,:2]\n",
    "# Y = np.array(data)[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(100000):\n",
    "    x , y = np.random.random() , np.random.random()\n",
    "    norme = np.square(x ** 2 + y ** 2)\n",
    "    angle = math.atan(y/x)\n",
    "    data.append([x,y,norme,angle])\n",
    "X = np.array(data)[:,:2]\n",
    "Y = np.array(data)[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial score:  0.8182065318623806\n",
      "epoch n°0 realized, training score: 0.07846004101615012, test score: 0.07832388071630904, time: 9.090514659881592\n",
      "epoch n°1 realized, training score: 0.06960240927186599, test score: 0.06967994823824569, time: 8.873120546340942\n",
      "epoch n°2 realized, training score: 0.06880796115167938, test score: 0.06881606065819056, time: 9.634108543395996\n",
      "epoch n°3 realized, training score: 0.06942525959070156, test score: 0.06971401119122868, time: 9.184272050857544\n",
      "epoch n°4 realized, training score: 0.06984652237761488, test score: 0.07006190243953674, time: 9.518407821655273\n",
      "epoch n°5 realized, training score: 0.06965905396931389, test score: 0.06929108695835487, time: 9.109460592269897\n",
      "epoch n°6 realized, training score: 0.08548790820498471, test score: 0.08478612315186143, time: 9.125425100326538\n",
      "epoch n°7 realized, training score: 0.08540611319905687, test score: 0.08515693336144162, time: 9.481502532958984\n",
      "epoch n°8 realized, training score: 0.06896657335210005, test score: 0.06997693996895066, time: 9.42364501953125\n",
      "epoch n°9 realized, training score: 0.06926923560813812, test score: 0.07018466113946878, time: 9.028676986694336\n"
     ]
    }
   ],
   "source": [
    "nn.train(X,Y,epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.41052309, 0.38017066]),\n",
       " array([[0.10411922, 0.74336785]]),\n",
       " array([0.0980059 , 0.74702991]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0],nn.predict([X[0]]),Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.backward(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.98929607, 0.56288807]),\n",
       " {'layer_0': {'W': array([[0.00018588, 0.09916613],\n",
       "          [0.08459164, 0.08656696],\n",
       "          [0.07856979, 0.00026142]]),\n",
       "   'B': array([[0.05622521],\n",
       "          [0.02995821],\n",
       "          [0.01942281]]),\n",
       "   'g': <function __main__.sigmoid(x)>,\n",
       "   'g_prime': <function __main__.sigmoid_prime(x)>,\n",
       "   'shape': {'W': (3, 2),\n",
       "    'B': (3, 1),\n",
       "    'g_prime': (3, 1),\n",
       "    'input': (2, 1),\n",
       "    'output': (3, 1)}},\n",
       "  'layer_1': {'W': array([[0.05346489, 0.09512572, 0.01652686],\n",
       "          [0.0272123 , 0.02087117, 0.08771289]]),\n",
       "   'B': array([[0.08309541],\n",
       "          [0.09143422]]),\n",
       "   'g': <function __main__.relu(X)>,\n",
       "   'g_prime': <function __main__.relu_prime(X)>,\n",
       "   'shape': {'W': (2, 3),\n",
       "    'B': (2, 1),\n",
       "    'g_prime': (2, 1),\n",
       "    'input': (3, 1),\n",
       "    'output': (2, 1)}}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0],nn.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_0': [array([[0.98929607],\n",
       "         [0.56288807]]),\n",
       "  array([[0.11222853],\n",
       "         [0.1623719 ],\n",
       "         [0.09729874]]),\n",
       "  array([[0.52802772],\n",
       "         [0.54050402],\n",
       "         [0.52430551]])],\n",
       " 'layer_1': [array([[0.52802772],\n",
       "         [0.54050402],\n",
       "         [0.52430551]]),\n",
       "  array([[0.17140731],\n",
       "         [0.16307238]]),\n",
       "  array([[0.17140731],\n",
       "         [0.16307238]])]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.forward_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_1': [array([[-1.59151958, -1.62912421, -1.58030054],\n",
       "         [-0.37408094, -0.38291977, -0.37144394]]),\n",
       "  array([[-3.0140834 ],\n",
       "         [-0.70844943]])],\n",
       " 'layer_0': [array([[-0.04448351, -0.02531016],\n",
       "         [-0.0740796 , -0.04214969],\n",
       "         [-0.02762335, -0.01571709]]),\n",
       "  array([[-0.04496481],\n",
       "         [-0.07488112],\n",
       "         [-0.02792223]])]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.backward_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74999\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.002075081946097534"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.train(X[:75000],Y[:75000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.002075081946097534, 0.0036048963836632646)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(X[:75000],Y[:75000]),nn.score(X[75000:],Y[75000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       ],\n",
       "       [0.5868795]])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum(0,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,1]) + np.array([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.dot(nn.layers[0]['W'] , X[0].reshape(2,1)) + nn.layers[0]['B']\n",
    "z = z.reshape(nn.layers[0]['shape']['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X[0].reshape((2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array([0.13445301]), array([1.24396206])],\n",
       "       [array([0.54997813]), array([1.73574006])],\n",
       "       [array([-1.0244631]), array([0.7470272])]], dtype=object)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.layers[0]['W']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "h = nn.layers[0]['g'](z).reshape(nn.layers[0]['shape']['output'])\n",
    "x = x.reshape(nn.layers[0]['shape']['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73105858],\n",
       "       [0.88079708]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.layers[0]['g'](np.array([1,2]).reshape(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
