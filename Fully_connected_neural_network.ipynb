{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(X):\n",
    "    return np.array([max(0,x) for x in X])\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def relu_prime(X):\n",
    "    return np.array([1 if x >= 0 else 0 for x in X])\n",
    "\n",
    "def activation(name):\n",
    "    if name =='sigmoid':\n",
    "        return sigmoid,sigmoid_prime\n",
    "    elif name=='relu':\n",
    "        return relu,relu_prime\n",
    "    else:\n",
    "        raise Exception('activation function not in [sigmoid,relu]')\n",
    "        \n",
    "def mean_square_error(y_hat,y):\n",
    "    return np.square(y_hat - y).mean(axis = 1)\n",
    "\n",
    "def mean_square_error_der(y_hat,y):\n",
    "    if len(y) > 1:\n",
    "        return 2*(y_hat - y).mean(axis = 1)\n",
    "    else:\n",
    "        return 2*(y_hat - y)\n",
    "\n",
    "def square_sum(X):\n",
    "    return np.square(X).sum()\n",
    "\n",
    "def loss_function(name):\n",
    "    if name=='mean_square_error':\n",
    "        return mean_square_error , mean_square_error_der\n",
    "    else:\n",
    "        raise Exception('loss function not in [mean_square_error]')\n",
    "\n",
    "def penalization(name):\n",
    "    if name == 'square_sum':\n",
    "        return square_sum\n",
    "    else:\n",
    "        raise Exception('penalization function not in [square_sum]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,layers_parameters , global_parameters):\n",
    "        self.layers = {l:{'W' : 0.1*np.random.random(size = (layers[l]['output_dim'],layers[l]['input_dim'])),\n",
    "                          'B' : 0.1*np.random.random(size = (layers[l]['output_dim'])),\n",
    "                          'g' : activation(layers[l]['activation'])[0],\n",
    "                          'g_prime': activation(layers[l]['activation'])[1]\n",
    "                         } for l in layers}\n",
    "        \n",
    "        self.input_dim = global_parameters['input']\n",
    "        self.output_dim = global_parameters['output']\n",
    "        self.layers_dim = global_parameters['layers']\n",
    "        self.batch_size = global_parameters['batch']\n",
    "        self.lr = global_parameters['learning_rate']\n",
    "        \n",
    "        self.loss = loss_function(global_parameters['loss'])[0]\n",
    "        self.loss_der = loss_function(global_parameters['loss'])[1]\n",
    "        \n",
    "    def predict(self,X):\n",
    "        if len(X) != self.input_dim:\n",
    "            raise Exception('input of size {} instead of {}'.format(len(X),self.input_dim))\n",
    "        for n in range(self.layers_dim):\n",
    "            X = self.layers[n]['g'](np.dot(self.layers[n]['W'] , X) + self.layers[n]['B'])\n",
    "        return X\n",
    "    \n",
    "    def loss_calculation(self,x,y):\n",
    "        y_hat = self.predict(X)\n",
    "        return self.loss(y_hat,h)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        intermediary_step = []\n",
    "        if len(x) != self.input_dim:\n",
    "            raise Exception('input of size {} instead of {}'.format(len(x),self.input_dim))\n",
    "        \n",
    "        for n in range(self.layers_dim):\n",
    "            z = np.dot(self.layers[n]['W'] , x) + self.layers[n]['B']\n",
    "            h = self.layers[n]['g'](z)\n",
    "            intermediary_step.append([x , z , h])\n",
    "            x = np.array(h)\n",
    "            \n",
    "        return intermediary_step\n",
    "    \n",
    "    def backward(self , intermediary_step , y):\n",
    "        if len(intermediary_step) != self.layers_dim:\n",
    "            raise Exception('dimensions do not match')\n",
    "        \n",
    "        print(intermediary_step[-1][2],y)\n",
    "        last_gradient_h = self.loss_der(intermediary_step[-1][2] , y)\n",
    "        \n",
    "        for i in range(len(intermediary_step)):\n",
    "            n = len(intermediary_step) - i - 1\n",
    "            x , z , h = intermediary_step[n]\n",
    "            last_gradient_z = np.dot(last_gradient_h , self.layers[n]['g_prime'](z))\n",
    "            print(x)\n",
    "            x = x.reshape((2,1))\n",
    "            print(x.T)\n",
    "            last_gradient_w = np.dot(last_gradient_z , x.T)\n",
    "            last_gradient_b = last_gradient_z\n",
    "            last_gradient_h = np.dot(last_gradient_z,self.layers[n]['W'])\n",
    "            \n",
    "            self.layers[n]['W'] -= self.lr * last_gradient_w\n",
    "            self.layers[n]['B'] -= self.lr * last_gradient_b\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def train(self , X , Y):\n",
    "        if len(X) != len(Y):\n",
    "            raise Exception('X and Y dimension do not match')\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            x , y = X[i] , Y[i]\n",
    "            intermediary_step = self.forward(x)\n",
    "            self.backward(intermediary_step,np.array([y]))  \n",
    "        return score(self,X,Y)\n",
    "    \n",
    "    def score(self,X,Y):\n",
    "        mean_square = 0\n",
    "        for i in range(len(X)):\n",
    "            y = self.predict(X[i])\n",
    "            mean_square += np.square(y - Y[i])\n",
    "        return mean_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = {\n",
    "          0:{'input_dim' : 2,\n",
    "             'output_dim': 2,\n",
    "             'activation': 'relu'},\n",
    "          1:{'input_dim' : 2,\n",
    "             'output_dim': 1,\n",
    "             'activation': 'relu'}\n",
    "         }\n",
    "\n",
    "parameters = {'input': 2,\n",
    "              'output' : 1,\n",
    "              'layers': 2,\n",
    "              'loss': 'mean_square_error',\n",
    "              'penalization':'square_sum',\n",
    "              'batch':1,\n",
    "              'learning_rate':0.1}\n",
    "\n",
    "nn = NeuralNetwork(layers , parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(10000):\n",
    "    y = i%2\n",
    "    dist = np.random.normal(loc = 1 + i%2,scale = 0.2)\n",
    "    x0 = np.random.uniform(low = -np.sqrt(dist/2),high = np.sqrt(dist/2))\n",
    "    sign = -1 if np.random.random() > 0.5 else 1\n",
    "    x1 = sign*np.sqrt(dist - x0**2)\n",
    "    data.append([x0,x1,y])\n",
    "X = np.array(data)[:,:2]\n",
    "Y = np.array(data)[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07995246] [0.]\n",
      "[0.04349843 0.01379485]\n",
      "[[0.04349843 0.01379485]]\n",
      "[ 0.35315023 -0.64082286]\n",
      "[[ 0.35315023 -0.64082286]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,) and (2,2) not aligned: 1 (dim 0) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-300-4b924b07a404>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-297-2a5926a44820>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mintermediary_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediary_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-297-2a5926a44820>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, intermediary_step, y)\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mlast_gradient_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_gradient_z\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mlast_gradient_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlast_gradient_z\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mlast_gradient_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_gradient_z\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlast_gradient_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,) and (2,2) not aligned: 1 (dim 0) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "nn.train(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
