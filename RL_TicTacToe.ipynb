{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Reinforcement Learning Algorithms:\n",
    "- Tic Tac Toe game, with Value Function: This implementation required a high number of exploration before starting exploitation in order to avoid the agents to learn from always the same states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Environmnent\n",
    "\n",
    "def flatten(grid):\n",
    "    return tuple(grid[i,j] for i in range(3) for j in range(3))\n",
    "\n",
    "def is_finished(grid):\n",
    "    for i in range(0,3):\n",
    "        if list(grid[i,:]) in [[1,1,1],[-1,-1,-1]]:\n",
    "            return True\n",
    "\n",
    "        if list(grid[:,i]) in [[1,1,1],[-1,-1,-1]]:\n",
    "            return True\n",
    "            \n",
    "        diag_1 = [grid[0,0],grid[1,1],grid[2,2]]\n",
    "        diag_2 = [grid[0,2],grid[1,1],grid[2,0]]\n",
    "        \n",
    "        if diag_1 in [[1,1,1],[-1,-1,-1]] or diag_2 in [[1,1,1],[-1,-1,-1]]:\n",
    "            return True\n",
    "        \n",
    "        if len(grid.nonzero()[0])==9:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "class Morpion(object):\n",
    "    def __init__(self):\n",
    "        self.field = np.zeros((3,3))\n",
    "        \n",
    "    def is_finished(self,state):\n",
    "        for i in range(0,3):\n",
    "            if list(self.field[i,:]) in [[1,1,1],[-1,-1,-1]]:\n",
    "                return True,state,1\n",
    "            if list(self.field[:,i]) in [[1,1,1],[-1,-1,-1]]:\n",
    "                return True,state,1\n",
    "    \n",
    "        diag_1 = [self.field[0,0],self.field[1,1],self.field[2,2]]\n",
    "        diag_2 = [self.field[0,2],self.field[1,1],self.field[2,0]]    \n",
    "        if diag_1 in [[1,1,1],[-1,-1,-1]] or diag_2 in [[1,1,1],[-1,-1,-1]]:\n",
    "            return True,state,1\n",
    "        \n",
    "        if len(self.field.nonzero()[0])==9:\n",
    "            return True,state,0.5\n",
    "        \n",
    "        return False,state,0\n",
    "    \n",
    "    def step(self,state,move,player,render = False):\n",
    "        n_state = state.copy()\n",
    "        n_state[move] = player.value\n",
    "        self.field = n_state.copy()\n",
    "        return game.is_finished(n_state)\n",
    "        \n",
    "    def reset(self):\n",
    "        state = np.zeros((3,3))\n",
    "        self.field = state.copy()\n",
    "        return state\n",
    "        \n",
    "    def display(self):\n",
    "        print(self.field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agent ###\n",
    "class Agent(object):\n",
    "    def __init__(self, is_human, shape, trainable = True):\n",
    "        self.is_human = is_human\n",
    "        self.size = shape\n",
    "        self.trainable = trainable\n",
    "        self.eps = 0.99\n",
    "        self.history = []\n",
    "        self.known_states = []\n",
    "        self.V = {}\n",
    "        self.rewards = []\n",
    "        self.win_nb = 0\n",
    "        self.lose_nb = 0\n",
    "        self.draw_nb = 0\n",
    "        self.value = 0\n",
    "        self.show = False\n",
    "        \n",
    "    def greedy_step(self,state):\n",
    "        actions = [(i,j) for i in range(0,3) for j in range(0,3) if state[(i,j)] == 0]\n",
    "        vmin = None\n",
    "        vi = None\n",
    "        \n",
    "        state = np.array(self.value * state)\n",
    "        for i in range(len(actions)):\n",
    "            a = actions[i]\n",
    "            temp = np.zeros((3,3))\n",
    "            temp[a] = 1\n",
    "            fl = flatten( - 1 * ( temp + state ) )\n",
    "            if  fl not in self.V:\n",
    "                self.V[fl] = 0\n",
    "                \n",
    "            if self.show:\n",
    "                print(fl)\n",
    "                print(self.V[fl])\n",
    "                \n",
    "            if vmin is None or vmin > self.V[fl]:\n",
    "                vmin = self.V[fl]\n",
    "                vi = i\n",
    "        return actions[vi]\n",
    "    \n",
    "    \n",
    "    def play(self,state):\n",
    "        if self.is_human:\n",
    "            inp = input(prompt = 'x y')\n",
    "            move = (int(inp.split()[0]),int(inp.split()[1]))\n",
    "        else:\n",
    "            if np.random.random() < self.eps:\n",
    "                actions = [(i,j) for i in range(0,3) for j in range(0,3) if state[(i,j)] == 0]\n",
    "                move = random.choice(actions)\n",
    "            else:\n",
    "                move = self.greedy_step(state)\n",
    "        return move\n",
    "                \n",
    "    def add_transition(self,n_tuples):\n",
    "        self.history.append(n_tuples)\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        if not self.trainable or self.is_human:\n",
    "            return\n",
    "        \n",
    "        for transition in reversed(self.history):\n",
    "            s, a, r, sp = transition\n",
    "            \n",
    "            if r == 0:\n",
    "                fl_s = flatten(self.value * s)\n",
    "                fl_sp = flatten(self.value * sp)\n",
    "                if fl_s not in self.V:\n",
    "                    self.V[fl_s] = 0 \n",
    "                if fl_sp not in self.V:\n",
    "                    self.V[fl_sp] = 0\n",
    "                self.V[fl_s] = self.V[fl_s] + 0.01 * ( self.V[fl_sp] - self.V[fl_s] )\n",
    "            \n",
    "            else:\n",
    "                fl_s = flatten(self.value * s)\n",
    "                if fl_s not in self.V:\n",
    "                    self.V[fl_s] = 0 \n",
    "                self.V[fl_s] = self.V[fl_s] + 0.01 * ( r - self.V[fl_s])\n",
    "        \n",
    "        self.history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Game ###\n",
    "\n",
    "def play(game,p1,p2,train = True):\n",
    "    state = game.reset()\n",
    "    p1.history = []\n",
    "    p2.history = []\n",
    "    players = [p1,p2]\n",
    "    random.shuffle(players)\n",
    "    players[0].value = 1\n",
    "    players[1].value = -1\n",
    "    \n",
    "    p=0\n",
    "    done = False\n",
    "    \n",
    "    while done is False:\n",
    "        \n",
    "        if players[p%2].is_human:\n",
    "            game.display()\n",
    "        \n",
    "        action = players[p%2].play(state)\n",
    "        done,n_state,reward = game.step(state,action,players[p%2])\n",
    "        \n",
    "        if done:\n",
    "            players[p%2].win_nb +=1 if reward == 1 else 0\n",
    "            players[(p + 1)%2].lose_nb +=1 if reward == 1 else 0\n",
    "            players[p%2].draw_nb +=1 if reward == 0.5 else 0 \n",
    "            players[(p + 1)%2].draw_nb +=1 if reward == 0.5 else 0\n",
    "            \n",
    "        if p != 0:\n",
    "            s, a, r, _ = players[(p + 1) % 2].history[-1]\n",
    "            if reward==0.5:\n",
    "                players[(p + 1) % 2].history[-1] = ( s, a, reward , np.array(n_state))\n",
    "            else:\n",
    "                players[(p + 1) % 2].history[-1] = ( s, a, reward * -1 , np.array(n_state))\n",
    "        \n",
    "        players[p%2].add_transition((np.array(state), action, reward, None))\n",
    "        state = n_state.copy()\n",
    "        \n",
    "        p += 1\n",
    "    \n",
    "    if p1.show:\n",
    "        print(p1.history)\n",
    "    if train:\n",
    "        p1.train()\n",
    "        p2.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Script for training ###\n",
    "game = Morpion()\n",
    "p1 = Agent(is_human = False,shape = (3,3),trainable = True)\n",
    "p2 = Agent(is_human = False,shape = (3,3),trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990000/1000000, new epsilon: 0.050036622866325645\r"
     ]
    }
   ],
   "source": [
    "n = 1000000\n",
    "\n",
    "for i in range(n):\n",
    "    if i%10000==0:\n",
    "        p1.eps = max(p1.eps*0.99,0.05)\n",
    "        p2.eps = max(p2.eps*0.99,0.05)\n",
    "        print('{}/{}, new epsilon: {}'.format(i,n,p1.eps),end = '\\r')\n",
    "        \n",
    "    play(game,p1,p2,train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842764, 914977, 1242259, 0.05, 914977, 842764, 1242259, 0.05)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.win_nb,p1.lose_nb,p1.draw_nb,p1.eps,p2.win_nb,p2.lose_nb,p2.draw_nb,p1.eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "x y1 1\n",
      "[[ 0.  1.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "x y2 2\n",
      "[[ 1.  1.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0. -1.]]\n",
      "x y0 2\n"
     ]
    }
   ],
   "source": [
    "p1.show = False\n",
    "h1 = Agent(is_human = True,shape = (3,3),trainable = True)\n",
    "play(game,p1,h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'epochs': 4000000,\n",
       "  'decay': '0.99 every 20000 steps',\n",
       "  'optimal_behaviour': True},\n",
       " 2: {'epochs': 2000000,\n",
       "  'decay': '0.99 every 10000 steps',\n",
       "  'optimal_behaviour': True}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{1:{'epochs':4000000, 'decay':'0.99 every 20000 steps','optimal_behaviour': True}\n",
    ",2:{'epochs':2000000, 'decay':'0.99 every 10000 steps','optimal_behaviour': True}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
